{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O₃ (Ozone) Pollutant Prediction Model\n",
    "\n",
    "This notebook details the end-to-end process for training a machine learning model to predict O₃ concentrations. The workflow includes:\n",
    "1. **Setup**: Importing libraries and defining utility functions.\n",
    "2. **Data Generation**: Creating a dummy dataset for demonstration.\n",
    "3. **Feature Engineering**: Building time-based, physical, and error-correction features.\n",
    "4. **Model Training**: Training a LightGBM regressor.\n",
    "5. **Evaluation**: Assessing model performance on a holdout test set.\n",
    "6. **Prediction**: Applying the trained model to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Notebook settings\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    test_size: float = 0.25\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Define local file paths\n",
    "DATAFILE = \"site_1_train_data.csv\"\n",
    "UNSEEN_DATA_FILE = \"site_1_unseen_input_data.csv\"\n",
    "MODEL_DIR = \"models_O3_high_performance\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dummy Data Generation\n",
    "\n",
    "This cell creates mock `train` and `unseen` data files, allowing the notebook to run from start to finish without external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data(filename, num_rows):\n",
    "    \"\"\"Creates a dummy CSV file with the required schema.\"\"\"\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_rows, freq='h'))\n",
    "    data = {\n",
    "        'year': dates.year, 'month': dates.month, 'day': dates.day, 'hour': dates.hour,\n",
    "        'O3_forecast': 50 + np.random.randn(num_rows) * 20 + 20 * np.sin(np.arange(num_rows) * 2 * np.pi / 24),\n",
    "        'NO2_forecast': 40 + np.random.randn(num_rows) * 15 - 15 * np.sin(np.arange(num_rows) * 2 * np.pi / 24),\n",
    "        'T_forecast': 25 + np.random.randn(num_rows) * 5,\n",
    "        'u_forecast': np.random.randn(num_rows) * 2,\n",
    "        'v_forecast': np.random.randn(num_rows) * 2,\n",
    "    }\n",
    "    if 'train' in filename:\n",
    "        data['O3_target'] = (data['O3_forecast'] + np.random.randn(num_rows) * 5).clip(lower=0)\n",
    "        data['NO2_target'] = (data['NO2_forecast'] + np.random.randn(num_rows) * 5).clip(lower=0)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['O3_forecast'] = df['O3_forecast'].clip(lower=0)\n",
    "    df['NO2_forecast'] = df['NO2_forecast'].clip(lower=0)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Created dummy data file: {filename}\")\n",
    "\n",
    "# Create the files\n",
    "create_dummy_data(DATAFILE, num_rows=8760) # 1 year of data\n",
    "create_dummy_data(UNSEEN_DATA_FILE, num_rows=168) # 1 week of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ria_score(y_true, y_pred):\n",
    "    \"\"\"Calculates the Refined Index of Agreement (RIA).\"\"\"\n",
    "    numerator = np.sum(np.abs(y_true - y_pred))\n",
    "    denominator = 2 * np.sum(np.abs(y_true - np.mean(y_true)))\n",
    "    return 1.0 if denominator == 0 else 1 - (numerator / denominator)\n",
    "\n",
    "def detailed_evaluation(y_true, y_pred, name=\"Dataset\"):\n",
    "    \"\"\"Calculates and prints detailed evaluation metrics.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    ria = ria_score(y_true, y_pred)\n",
    "    print(f\"\\n=== {name} Metrics ===\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2: {r2:.4f}\")\n",
    "    print(f\"RIA: {ria:.4f}\")\n",
    "    print(f\"Prediction Range: Min={y_pred.min():.4f}, Max={y_pred.max():.4f}\")\n",
    "\n",
    "def time_aware_split(X, y, timestamps, test_size):\n",
    "    \"\"\"Splits time-series data chronologically.\"\"\"\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:], timestamps[:split_idx], timestamps[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df: pd.DataFrame) -> (pd.DataFrame, List[str]):\n",
    "    df = df.copy()\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "        df = df.set_index('datetime').sort_index()\n",
    "\n",
    "    # Time and physical features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    for col in ['hour', 'dayofweek', 'month']:\n",
    "        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / df[col].max())\n",
    "        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / df[col].max())\n",
    "    df['wind_speed'] = np.sqrt(df['u_forecast']**2 + df['v_forecast']**2)\n",
    "\n",
    "    # Lag and rolling features\n",
    "    for col in ['O3_forecast', 'NO2_forecast', 'T_forecast', 'wind_speed']:\n",
    "        if col in df.columns:\n",
    "            for L in [1, 2, 3, 6, 12, 24]:\n",
    "                df[f'{col}_lag{L}'] = df[col].shift(L)\n",
    "            for W in [6, 12, 24]:\n",
    "                df[f'{col}_rolling_mean_{W}h'] = df[col].rolling(W, min_periods=1).mean()\n",
    "\n",
    "    # Error correction for O3\n",
    "    if 'O3_target' in df.columns:\n",
    "        df['forecast_error'] = df['O3_forecast'] - df['O3_target']\n",
    "        for L in [1, 2, 3, 6, 12, 24]:\n",
    "            df[f'forecast_error_lag{L}'] = df['forecast_error'].shift(L)\n",
    "\n",
    "    base_cols = [col for col in df.columns if 'target' not in col and 'forecast_error' not in col]\n",
    "    feature_cols = [col for col in base_cols if col not in ['year', 'day', 'hour', 'month', 'dayofweek', 'u_forecast', 'v_forecast']]\n",
    "    return df.ffill().bfill().fillna(0), feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "df, feature_cols = build_features(pd.read_csv(DATAFILE))\n",
    "train_feature_cols = feature_cols + [f'forecast_error_lag{L}' for L in [1, 2, 3, 6, 12, 24]]\n",
    "print(f\"Training O3 model with {len(train_feature_cols)} features.\")\n",
    "\n",
    "# Split data\n",
    "y = df[\"O3_target\"].ffill().bfill().values\n",
    "X = df[train_feature_cols].values\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = time_aware_split(X, y, df.index, config.test_size)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s, X_test_s = scaler.transform(X_train), scaler.transform(X_test)\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, \"scaler_O3.joblib\"))\n",
    "\n",
    "# Train model\n",
    "print(\"\\n--- Training O3 Model ---\")\n",
    "model_params = {\n",
    "    'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000, 'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'num_leaves': 128, 'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "}\n",
    "model = lgb.LGBMRegressor(**model_params)\n",
    "model.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)], eval_metric='rmse', callbacks=[lgb.early_stopping(100)])\n",
    "joblib.dump(model, os.path.join(MODEL_DIR, \"final_model_O3.joblib\"))\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\n--- Evaluating O3 Model ---\")\n",
    "final_preds = model.predict(X_test_s)\n",
    "detailed_evaluation(y_test, final_preds, name=\"O3 Holdout Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(filename, title, xlabel, ylabel):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if plt.gca().get_legend_handles_labels()[1]:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_DIR, filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nGenerating O3 evaluation plots...\")\n",
    "\n",
    "# Time-series plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(t_test, y_test, label=\"Actual\", alpha=0.8)\n",
    "plt.plot(t_test, final_preds, label=\"Predicted\", linestyle='--')\n",
    "save_plot(\"test_overlay_O3.png\", \"Actual vs. Predicted O3\", \"Datetime\", r\"O3 Concentration ($\\mu g/m^3$)\")\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, final_preds, alpha=0.5)\n",
    "lims = [min(np.nanmin(y_test), np.nanmin(final_preds)), max(np.nanmax(y_test), np.nanmax(final_preds))]\n",
    "plt.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label=\"Ideal\")\n",
    "plt.xlim(lims); plt.ylim(lims)\n",
    "save_plot(\"test_scatter_O3.png\", \"Predicted vs. Actual Scatter Plot\", \"Actual O3\", \"Predicted O3\")\n",
    "\n",
    "# Forecast vs. Predicted plot\n",
    "o3_forecast_test = df.loc[t_test, \"O3_forecast\"].values\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(t_test, o3_forecast_test, label=\"O3 Forecast\", alpha=0.8)\n",
    "plt.plot(t_test, final_preds, label=\"O3 Predicted (Corrected)\", linestyle='--', color='coral')\n",
    "save_plot(\"forecast_vs_predicted_test_O3.png\", \"O3 Forecast vs. Model Prediction (Test Data)\", \"Datetime\", r\"O3 Concentration ($\\mu g/m^3$)\")\n",
    "\n",
    "print(f\"O3 evaluation plots saved to '{MODEL_DIR}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(UNSEEN_DATA_FILE):\n",
    "    print(\"\\n--- Predicting O3 on unseen data ---\")\n",
    "    udf, unseen_feature_cols = build_features(pd.read_csv(UNSEEN_DATA_FILE))\n",
    "\n",
    "    X_unseen = udf[unseen_feature_cols].reindex(columns=train_feature_cols).fillna(0).values\n",
    "    X_unseen_s = scaler.transform(X_unseen)\n",
    "    final_unseen_preds = model.predict(X_unseen_s)\n",
    "\n",
    "    results = pd.DataFrame(index=udf.index, data={\n",
    "        'O3_forecast': udf['O3_forecast'],\n",
    "        'O3_predicted': final_unseen_preds\n",
    "    })\n",
    "    out_path = os.path.join(MODEL_DIR, \"unseen_predictions_O3.csv\")\n",
    "    results.to_csv(out_path)\n",
    "    print(f\"Saved unseen O3 predictions -> {out_path}\")\n",
    "\n",
    "    # Plot unseen data results\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(results.index, results['O3_forecast'], label=\"O3 Forecast\", alpha=0.8)\n",
    "    plt.plot(results.index, results['O3_predicted'], label=\"O3 Predicted (Corrected)\", linestyle='--')\n",
    "    save_plot(\"forecast_vs_predicted_unseen_O3.png\", \"O3 Forecast vs. Model Prediction (Unseen Data)\", \"Datetime\", r\"O3 Concentration ($\\mu g/m^3$)\")\n",
    "    print(f\"Unseen data plot saved to '{MODEL_DIR}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}